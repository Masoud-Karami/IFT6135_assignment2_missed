# IFT6135 Representation Learning_sequential language models
Word Level Models
  # Penn Treebank
Convolutional Neural Network
----------------------------
  - simple ("vanilla") RNN (recurrent neural network)
      - "Vanilla" Stochastic Gradient Descent (SGD)
      - SGD with a learning rate schedule
      - Adam
  - RNN with a gating mecahnism, Gated Recurrent Units (GRUs)
  - Attention Module of a Transformer Network
  - Training Language Models (cross-entropy loss as its performance) and report perplexity (PPL), which is
the exponentiated average per-token NLL (over all tokens)
  - Detailed evaluation of trained models
  
