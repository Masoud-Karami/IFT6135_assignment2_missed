# IFT6135 Representation Learning_sequential language models
Word Level Models
  # Penn Treebank
Convolutional Neural Network
----------------------------
  - simple ("vanilla") RNN (recurrent neural network)
      - "Vanilla" Stochastic Gradient Descent (SGD)
      - SGD with a learning rate schedule
      - Adam
  - RNN with a gating mecahnism, Gated Recurrent Units (GRUs)
  - Attention Module of a Transformer Network
  - Training Language Models (cross-entropy loss as its performance) and report perplexity (PPL), which is
the exponentiated average per-token NLL (over all tokens)
  - Detailed evaluation of trained models
  
more details and .py codes\
  - https://github.com/sebastianruder/NLP-progress/blob/master/english/language_modeling.md\
  - https://github.com/JonathanRaiman/pytreebank/blob/master/README.md
  - https://arxiv.org/pdf/1708.02709.pdf         arXiv:1708.02709v8 [cs.CL] 25 Nov 2018
  - https://arxiv.org/pdf/1902.02380.pdf         
